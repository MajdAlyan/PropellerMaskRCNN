{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036d9a8d-6653-42aa-a8c5-fa49cfb5e4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMask R-CNN wird mit Tipvortexcavitationdatensatz trainiert. Farbspritzereffekt(color splash effect) wird im Algorithmus Implementiert\\n\\n------------------------------------------------------------\\n\\n# Ein neues Modell wird mit den vortrainierten Coco-Gewichten trainiert\\npython3 Tipvortexcavitation.py train --dataset=/path/to/Tipvortexcavitation/dataset --weights=coco \\nfür Linux der Pfad und die Gewichte von Coco dataset\\npython  Tipvortexcavitation.py train --dataset=C:/Users/majd4/Desktop/Bachelorarbeit/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/datasets/Tipvortexcavitation --weights=coco\\n\\n# Fortsetzen des Trainierens von einem Modell, was schon trainiert wurde \\npython3 Tipvortexcavitation.py train --dataset=/path/to/Tipvortexcavitation/dataset --weights=last für Linux \\npython  Tipvortexcavitation.py train --dataset=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/datasets/Tipvortexcavitation --weights=last\\n\\n# Ein neues Modell wird mit den ImageNet-Gewichten trainiert\\npython3 Tipvortexcavitation.py train --dataset=/path/to/Tipvortexcavitation/dataset --weights=imagenet für Linux\\npython  Tipvortexcavitation.py train --dataset=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/datasets/Tipvortexcavitation --weights=imagenet\\n    \\n# Farbspritzer(color splash) auf ein Bild anwenden\\npython3 Tipvortexcavitation.py splash --weights=/path/to/weights/file.h5 --image=<URL or path to file> für Linux\\npython Tipvortexcavitation.py  splash --weights=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/mask_rcnn_tipvortexcavitation_0020.h5 --image=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/111.jpg\\n    \\n# Farbspritzer (color splash) anwenden mit den zuletzt trainierten Gewichten\\npython3 Tipvortexcavitation.py splash --weights=last --video=<URL or path to file> für Linux\\npython  Tipvortexcavitation.py splash --weights=last --video=<URL or path to file>  für Windows\\npython Tipvortexcavitation.py splash --weights=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/mask_rcnn_tipvortexcavitation_0020.h5 --video=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/11.mp4 \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mask R-CNN wird mit Tipvortexcavitationdatensatz trainiert. Farbspritzereffekt(color splash effect) wird im Algorithmus Implementiert\n",
    "\n",
    "------------------------------------------------------------\n",
    "\n",
    "# Ein neues Modell wird mit den vortrainierten Coco-Gewichten trainiert\n",
    "python3 Tipvortexcavitation.py train --dataset=/path/to/Tipvortexcavitation/dataset --weights=coco \n",
    "für Linux der Pfad und die Gewichte von Coco dataset\n",
    "python  Tipvortexcavitation.py train --dataset=C:/Users/majd4/Desktop/Bachelorarbeit/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/datasets/Tipvortexcavitation --weights=coco\n",
    "\n",
    "# Fortsetzen des Trainierens von einem Modell, was schon trainiert wurde \n",
    "python3 Tipvortexcavitation.py train --dataset=/path/to/Tipvortexcavitation/dataset --weights=last für Linux \n",
    "python  Tipvortexcavitation.py train --dataset=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/datasets/Tipvortexcavitation --weights=last\n",
    "\n",
    "# Ein neues Modell wird mit den ImageNet-Gewichten trainiert\n",
    "python3 Tipvortexcavitation.py train --dataset=/path/to/Tipvortexcavitation/dataset --weights=imagenet für Linux\n",
    "python  Tipvortexcavitation.py train --dataset=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/datasets/Tipvortexcavitation --weights=imagenet\n",
    "    \n",
    "# Farbspritzer(color splash) auf ein Bild anwenden\n",
    "python3 Tipvortexcavitation.py splash --weights=/path/to/weights/file.h5 --image=<URL or path to file> für Linux\n",
    "python Tipvortexcavitation.py  splash --weights=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/mask_rcnn_tipvortexcavitation_0020.h5 --image=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/111.jpg\n",
    "    \n",
    "# Farbspritzer (color splash) anwenden mit den zuletzt trainierten Gewichten\n",
    "python3 Tipvortexcavitation.py splash --weights=last --video=<URL or path to file> für Linux\n",
    "python  Tipvortexcavitation.py splash --weights=last --video=<URL or path to file>  für Windows\n",
    "python Tipvortexcavitation.py splash --weights=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/mask_rcnn_tipvortexcavitation_0020.h5 --video=D:/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN/11.mp4 \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8737683-3451-4422-b093-c7ffcb334f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "\n",
    "# der Pfad der Datei von dem Mask R-CNN Projekt \n",
    "ROOT_DIR = os.path.abspath(\"C:/Users/majd4/Desktop/Bachelorarbeit/Bachelor-Arbeit-Daten/MaskRCNNProjekt/MaskRCNN_2/Mask_RCNN\")\n",
    "\n",
    "# Maske RCNN importieren\n",
    "sys.path.append(ROOT_DIR)  \n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "from mrcnn import visualize\n",
    "\n",
    "# Pfad zur Datei mit den trainierten Gewichten\n",
    "# was vorher trainiert wurde, natürlich wird es nochmals benutzt, um ein neues Modell zu trainieren \n",
    "# es wird das neue Modell besser in der erkennung von Tipvortexcavitation\n",
    "WEIGHTSS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_tipvortexcavitation_0010.h5\")\n",
    "\n",
    "# hier ist die Datei, wo die logs gespeichert wurden \n",
    "# Epochen von dem Training \n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "############################################################\n",
    "#  Konfigurationen\n",
    "############################################################\n",
    "\n",
    "# Esi ist hilfreich diese Website zu lesen, wenn man Änderung auf den Code vornehmen möchte \n",
    "#  https://github.com/matterport/Mask_RCNN/wiki\n",
    "\n",
    "\n",
    "class TipvortexcavitationConfig(Config):\n",
    "    \"\"\"Konfiguration für das trainieren mit dem Tipvortexcavitationdatensatz\n",
    "    Abgeleitet von dem basic Config Klasse und werden einige Werte überschrieben \n",
    "    \"\"\"\n",
    "    # ANZAHL DER zu verwendenden GPUs. Wenn nur eine CPU verwendet wird, muss dies auf 1 gesetzt werden.\n",
    "    # es ist abhängig von dem Prozessor und die Grafikkarte, die man hat \n",
    "    # hier Auf meinem Laptop nutze ich AMD Ryzen 7 3700U with Radeon Vega Mobile Gfx 2.30 GHz\n",
    "    GPU_COUNT = 1\n",
    "    \n",
    "    #Die Konfiguration wird einen erkennbaren Namen gegeben \n",
    "    NAME = \"Tipvortexcavitation\"\n",
    "\n",
    "    # verwende eine GPU mit 12 GB Speicher, die zwei Bilder aufnehmen kann.\n",
    "    # Passe nach unten an, wenn eine kleinere GPU verwendet wird.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Anzahl der Klassen (einschließlich Hintergrund)\n",
    "    NUM_CLASSES = 1 + 1  # Background + Tipvortexcavitation\n",
    "    \n",
    "    # Anzahl der Farbkanäle pro Bild.  \n",
    "    # RGB = 3 (Bilder nur mit Farbe), \n",
    "    # grayscale = 1 (Bilder nur Schwarz und Weiß), \n",
    "    # RGB-D = 4 ( Bilder werden von einer Speziellen Kamera aufgenommen. \n",
    "    # jedes Pixel im Bildem ist mit einem Intensitätswert verbunden)\n",
    "    IMAGE_CHANNEL_COUNT = 3\n",
    "    \n",
    "    \n",
    "    # Backbone Netzwerk die Architektur\n",
    "    # Unterstützte Werte sind: resnet50, resnet101.\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # Anzahl der Trainingsschritte pro Epoche\n",
    "    STEPS_PER_EPOCH = 10\n",
    "    \n",
    "    # Anzahl der Validierungsschritte, die am Ende jeder Trainingsepoche ausgeführt werden.\n",
    "    \n",
    "    VALIDATION_STEPS = 50\n",
    "\"\"\"\n",
    "class Config(object):\n",
    "    Base configuration class. For custom configurations, create a\n",
    "    sub-class that inherits from this one and override properties\n",
    "    that need to be changed.\n",
    "    \n",
    "    # Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.\n",
    "    # Useful if your code needs to do things differently depending on which\n",
    "    # experiment is running.\n",
    "    NAME = None  # Override in sub-classes\n",
    "\n",
    "    # NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.\n",
    "    GPU_COUNT = 1\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 2\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "    # Number of validation steps to run at the end of every training epoch.\n",
    "    # A bigger number improves accuracy of validation stats, but slows\n",
    "    # down the training.\n",
    "    VALIDATION_STEPS = 50\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101.\n",
    "    # You can also provide a callable that should have the signature\n",
    "    # of model.resnet_graph. If you do so, you need to supply a callable\n",
    "    # to COMPUTE_BACKBONE_SHAPE as well\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # Only useful if you supply a callable to BACKBONE. Should compute\n",
    "    # the shape of each layer of the FPN Pyramid.\n",
    "    # See model.compute_backbone_shapes\n",
    "    COMPUTE_BACKBONE_SHAPE = None\n",
    "\n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a Resnet101 backbone.\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "\n",
    "    # Size of the fully-connected layers in the classification graph\n",
    "    FPN_CLASSIF_FC_LAYERS_SIZE = 1024\n",
    "\n",
    "    # Size of the top-down layers used to build the feature pyramid\n",
    "    TOP_DOWN_PYRAMID_SIZE = 256\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1  # Override in sub-classes\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "    # Anchor stride\n",
    "    # If 1 then anchors are created for each cell in the backbone feature map.\n",
    "    # If 2, then anchors are created for every other cell, and so on.\n",
    "    RPN_ANCHOR_STRIDE = 1\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 256\n",
    "    \n",
    "    # ROIs kept after tf.nn.top_k and before non-maximum suppression\n",
    "    PRE_NMS_LIMIT = 6000\n",
    "\n",
    "    # ROIs kept after non-maximum suppression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 1000\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Input image resizing\n",
    "    # Generally, use the \"square\" resizing mode for training and predicting\n",
    "    # and it should work well in most cases. In this mode, images are scaled\n",
    "    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n",
    "    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n",
    "    # padded with zeros to make it a square so multiple images can be put\n",
    "    # in one batch.\n",
    "    # Available resizing modes:\n",
    "    # none:   No resizing or padding. Return the image unchanged.\n",
    "    # square: Resize and pad with zeros to get a square image\n",
    "    #         of size [max_dim, max_dim].\n",
    "    # pad64:  Pads width and height with zeros to make them multiples of 64.\n",
    "    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n",
    "    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n",
    "    #         The multiple of 64 is needed to ensure smooth scaling of feature\n",
    "    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
    "    # crop:   Picks random crops from the image. First, scales the image based\n",
    "    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n",
    "    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n",
    "    #         IMAGE_MAX_DIM is not used in this mode.\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    # Minimum scaling ratio. Checked after MIN_IMAGE_DIM and can force further\n",
    "    # up scaling. For example, if set to 2 then images are scaled up to double\n",
    "    # the width and height, or more, even if MIN_IMAGE_DIM doesn't require it.\n",
    "    # However, in 'square' mode, it can be overruled by IMAGE_MAX_DIM.\n",
    "    IMAGE_MIN_SCALE = 0\n",
    "    # Number of color channels per image. RGB = 3, grayscale = 1, RGB-D = 4\n",
    "    # Changing this requires other changes in the code. See the WIKI for more\n",
    "    # details: https://github.com/matterport/Mask_RCNN/wiki\n",
    "    IMAGE_CHANNEL_COUNT = 3\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([123.7, 116.8, 103.9])\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Percent of positive ROIs used to train classifier/mask heads\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "\n",
    "    # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "\n",
    "    # Shape of output mask\n",
    "    # To change this you also need to change the neural network mask branch\n",
    "    MASK_SHAPE = [28, 28]\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 100\n",
    "\n",
    "    # Bounding box refinement standard deviation for RPN and final detections.\n",
    "    RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "\n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "\n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n",
    "    # weights to explode. Likely due to differences in optimizer\n",
    "    # implementation.\n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "\n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "    # Loss weights for more precise optimization.\n",
    "    # Can be used for R-CNN training setup.\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": 1.\n",
    "    }\n",
    "\n",
    "    # Use RPN ROIs or externally generated ROIs for training\n",
    "    # Keep this True for most situations. Set to False if you want to train\n",
    "    # the head branches on ROI generated by code rather than the ROIs from\n",
    "    # the RPN. For example, to debug the classifier head without having to\n",
    "    # train the RPN.\n",
    "    USE_RPN_ROIS = True\n",
    "\n",
    "    # Train or freeze batch normalization layers\n",
    "    #     None: Train BN layers. This is the normal mode\n",
    "    #     False: Freeze BN layers. Good when using a small batch size\n",
    "    #     True: (don't use). Set layer in training mode even when predicting\n",
    "    TRAIN_BN = False  # Defaulting to False since batch size is often small\n",
    "\n",
    "    # Gradient norm clipping\n",
    "    GRADIENT_CLIP_NORM = 5.0\n",
    "    \n",
    "\"\"\" \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
